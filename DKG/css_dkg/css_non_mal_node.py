import sys, json, re, time, csv
import os, threading, socket, ast
import numpy as np

from charm.core.engine.util import *
from charm.core.math.integer import *

from OpenSSL import SSL, crypto
from sys import argv
from time import sleep
from operator import add

sys.path += ['./', '../', '../../']

from conf.connectionconfig import *
from conf.groupparam import *
from util.connectionutils import *
from util.transactionbroadcast import *
from util.nizk import *
from secretsharing.combinatorial.cssutils.css_commitment import *

# from dprf              import partial_eval


debug = 0
BASE_PORT = 6566
MY_IP = "127.0.0.1"

MALICIOUS = 0

broadcast_counter = 0

peers = {}
connections = {}

my_rcvd_shares = {}
my_rcvd_shares_dash = {}
my_rcvd_shares_strings = {}
my_rcvd_shares_dash_strings = {}

peer_share_commits = {}
peer_dlog_commits = {}

generated_shares = {}

complaints = {}
records = {}
nizks = {}

accused_nodes = []

nodes_verification_failed = []
QualifiedSet = []
DisqualifiedSet = []

tx_count = 0
epoch = 0

g_rand = group.random(G)
zero = group.init(ZR, int(0))
unity = g_rand ** zero


def DPRINT(*args, **kwargs):
    if debug:
        print(*args, **kwargs)


def deserializeElements(objects):
    object_byte_strings = re.findall(r"'(.*?)'", objects, re.DOTALL)
    object_strings = [str.encode(a) for a in object_byte_strings]
    elements = [group.deserialize(a) for a in object_strings]
    return elements


def verifyConnection(conn, cert, errnum, depth, ok):
    # print("Got Certificate from ", str(conn))
    return ok


def initSSLContext():
    ctx = SSL.Context(SSL.SSLv23_METHOD)
    ctx.set_options(SSL.OP_NO_SSLv2)
    ctx.set_options(SSL.OP_NO_SSLv3)
    ctx.set_verify(
        SSL.VERIFY_PEER, verifyConnection
    )  # Demand a Certificate
    ctx.use_privatekey_file(CLIENT_PVT_KEY_FILE)
    ctx.use_certificate_file(CLIENT_CERT_FILE)
    ctx.load_verify_locations(CA_FILE)
    return ctx


def serverSock(MY_IP, MY_PORT):
    ctx = initSSLContext()
    s = SSL.Connection(ctx, socket.socket(socket.AF_INET, socket.SOCK_STREAM))
    s.bind(('', MY_PORT))
    s.listen(N_NODES)
    for peer in range(N_NODES - 1):
        try:
            peer_con, peer_addr = s.accept()
            pid = recvInt(peer_con)  # TODO: change this from int to data directly
            print("Received Hello from the node ", pid, " at ", str(peer_addr))
            peers[pid] = peer_con
            # TODO: add acknowledgement
        except Exception as e:
            print("Exception in receiving hello from another node", pid, ":", e)


def sendId2peers(id):
    ctx = initSSLContext()

    for node_index in range(N_NODES):
        if (node_index != id):
            try:
                DPRINT("Attempting to send Hello to Node", node_index)
                s = SSL.Connection(ctx, socket.socket(socket.AF_INET, socket.SOCK_STREAM))
                s.connect(("127.0.0.1", BASE_PORT + node_index))
                connections[node_index] = s
                DPRINT("Sending Hello to PORT", BASE_PORT + node_index, " of Node", node_index)
                sendInt(s, id)
            except Exception as e:
                print("Exception in sending hello to node", node_index, ":", e)


def sendShareCommits2Peers(M, id):
    global tx_count
    global epoch
    tx_count = tx_count + 1

    ctx = initSSLContext()
    # S, S_dash , rho_commits, rho_commit_strings, RHO, RHO_dash, dlog_commits, dlog_commit_strings  = rhoCommit(M)
    if MALICIOUS:
        S, S_dash, share_commits, share_commit_strings, dlog_commits, dlog_commit_strings = cssCommit(M, MALICIOUS)
    else:
        S, S_dash, share_commits, share_commit_strings, dlog_commits, dlog_commit_strings = cssCommit(M)
    querykey = "ID" + str(id) + "tx_count" + str(tx_count) + "epoch" + str(epoch) + str(time.strftime("%Y-%m-%d-%H-%M"))
    DPRINT(querykey)

    # Save random shares generated by self

    S_strings = []
    S_dash_strings = []
    for i in range(len(S)):
        S_strings.append(group.serialize(S[i]))
        S_dash_strings.append(group.serialize(S_dash[i]))

    generated_shares['S'] = str(S)
    generated_shares['S_dash'] = str(S_dash)
    generated_shares['S_strings'] = str(S_strings)
    generated_shares['S_dash_strings'] = str(S_dash_strings)
    generated_shares['PedersenCommits'] = str(share_commits)
    generated_shares['PedersenCommitStrings'] = str(share_commit_strings)
    # generated_shares['RHOStrings']             = str(RHO_strings)
    # generated_shares['RHODashStrings']         = str(RHO_dash_strings)
    generated_shares['DlogCommits'] = str(dlog_commits)
    generated_shares['DlogCommitStrings'] = str(dlog_commit_strings)

    ############# Broadcast using Tendermint #####################
    tobdx = {'my_id': id, 'BroadcastCommit': str(share_commit_strings), 'epoch': 0}
    broadcast(tobdx, querykey)

    DPRINT(S)
    # either send to the stored PID or just send to the node list
    # here sending to each stored node in the peer list

    DPRINT("printing peer list", peers)

    # N_M_map = json.load(open("../../secretsharing/combinatorial/tmp/N_M_map.txt"))
    node_share_index = json.load(open("../../secretsharing/combinatorial/tmp/css_node_share_index.txt"))

    # DPRINT("N_M_map", N_M_map)
    DPRINT("node_share_index", node_share_index)
    DPRINT("node_share_index.keys()", node_share_index.keys())

    for i in range(len(node_share_index.keys())):
        DPRINT("node_share_index[i]", node_share_index[str(i)])

    for pid in list(connections.keys()):
        records[pid] = {}

        # converting elements to strings before sending

        DPRINT("pid:", pid)
        DPRINT("node_share_index[pid]", node_share_index[str(pid)])

        shares = []
        shares_dash = []

        shares_strings = []
        shares_dash_strings = []

        for index in node_share_index[str(pid)]:
            shares.append(S[index])
            shares_dash.append(S_dash[index])

            shares_strings.append(group.serialize(S[index]))
            shares_dash_strings.append(group.serialize(S_dash[index]))

        # shares = S[node_share_index[str(pid)]]
        DPRINT(shares)

        # data_to_send = {'my_id':id, 'rho_commits': str(rho_commits), 'share':shares }
        data_to_send = {'msg_type': "SHARES",
                        'my_id': id,
                        # 'rho_commits': str(rho_commits),
                        'share': str(shares),
                        'share_strings': str(shares_strings),
                        'share_dash_strings': str(shares_dash_strings),
                        'key': querykey}
        # data_to_send = {'my_id':id, 'rho_commits': str(rho_commits), 'share':str(S[pid])}
        data_to_send = json.dumps(data_to_send)

        # Store what is being sent for later usage during complaints
        records[pid]['SENT_SHARES'] = data_to_send

        DPRINT(data_to_send)
        send_data(connections[pid], data_to_send)


def receive_shares():
    # TODO: Add time-out

    for pid in peers.keys():
        share_commits_rcvd = recv_data(peers[pid])
        share_rcvd = json.loads(share_commits_rcvd)

        DPRINT("\nReceived something:\n", share_rcvd)

        # my_rcvd_shares[pid]= ast.literal_eval(share_rcvd['share'])[0][0]
        my_rcvd_shares[pid] = ast.literal_eval(share_rcvd['share'])
        DPRINT("My received shares", my_rcvd_shares)
        '''
        peer_share_commits[pid]= ast.literal_eval(share_rcvd['rho_commits'])
        '''

        # Store in strings from for complaint phase
        my_rcvd_shares_strings[pid] = share_rcvd['share_strings']
        my_rcvd_shares_dash_strings[pid] = share_rcvd['share_dash_strings']

        # Deserialize to obtain the values
        my_rcvd_shares[pid] = deserializeElements(share_rcvd['share_strings'])
        my_rcvd_shares_dash[pid] = deserializeElements(share_rcvd['share_dash_strings'])
        DPRINT("My received shares", my_rcvd_shares)

        query_key = share_rcvd['key']
        #############Query from Tendermint############

        queried_result = query(query_key)
        DPRINT("queried_result", queried_result)

        commits = queried_result['BroadcastCommit']
        final_commits = deserializeElements(commits)
        DPRINT(final_commits)

        DPRINT("\nExtracted the share", my_rcvd_shares[pid])
        DPRINT(type(my_rcvd_shares[pid][0]))
        peer_share_commits[pid] = final_commits


def add_received_shares():
    return


def verify_received_shares(n, id):
    # M = np.loadtxt("m3.txt", dtype=int)             #TODO: Change the filename to variable
    node_share_index = json.load(open("../../secretsharing/combinatorial/tmp/css_node_share_index.txt"))
    M_row_index_for_pid = node_share_index[str(id)]
    # M_my_rows = M[node_share_index[str(id)]]
    # print("My M rows:", M_my_rows)

    for pid in peers.keys():
        peer_commits = peer_share_commits[pid]
        shares_rcvd = my_rcvd_shares[pid]
        shares_dash_rcvd = my_rcvd_shares_dash[pid]

        '''
        if len(M_my_rows) != len(shares_rcvd):
            print("Eroor!: The number of nodes' rows in M and number of shares received are not same")

        com(s_i) = (C_i)**(m_{i,1}) * (C_i)**(m_{i,2}) ... * (C_i)**(m_{i,e})
        '''
        # g_rand = group.random(G)
        # g = group.encode(decoded_g)
        # h = group.encode(decoded_h)

        DPRINT("shares_rcvd", shares_rcvd)
        DPRINT("shares_dash_rcvd", shares_dash_rcvd)

        verified_shares_counter = 0

        # Check each received share
        for i in range(len(shares_rcvd)):
            # DPRINT("M_my_rows[",i,"]:", M_my_rows[i])
            DPRINT("peer_commits", peer_commits)
            computed_share_commitment = (g ** shares_rcvd[i]) * (h ** shares_dash_rcvd[i])

            if computed_share_commitment in peer_commits:
                DPRINT("Share[", i, "] Verified")
                verified_shares_counter += 1

            '''
            '''

        if (verified_shares_counter == len(shares_rcvd)):
            print("Great, all shares verified for peer ID:", pid)
        else:
            print("Something looks fishy, raising a complaint against peer ID:", pid)
            nodes_verification_failed.append(pid)


def broadcastFailedNodeList(nid):
    print("Broadcasting accusations now")

    print("nodes_verification_failed:", nodes_verification_failed)

    global tx_count
    global epoch
    tx_count = tx_count + 1
    accusation_query_key = ""

    total_accusations = []
    for pid in nodes_verification_failed:
        accusation = {}
        accusation['node_id'] = pid
        print("Accusing Node id:", pid)

        accusation['shares'] = my_rcvd_shares_strings[pid]
        accusation['shares_dash'] = my_rcvd_shares_dash_strings[pid]
        total_accusations.append(accusation)
    accusations_string = json.dumps(total_accusations)

    complaint = {'msg_type': "COMPLAINT",
                 'my_id': nid,
                 'accusation': accusations_string
                 # 'accusation': total_accusations
                 }
    complaint = dict(complaint)
    accusation_querykey = "AccusationFromID" + str(nid) + "tx_count" + str(tx_count) + "epoch" + str(epoch) + str(
        time.strftime("%Y-%m-%d-%H-%M"))

    ######### Broadcast the complaint using Tendermint 
    broadcast(complaint, accusation_querykey)

    ########## Send indication to all nodes
    for pid in list(connections.keys()):
        DPRINT("pid:", pid)

        yesorno = ""
        if pid in nodes_verification_failed:
            yesorno = "yes"
        else:
            yesorno = "no"

        data_to_send = {'msg_type': "COMPLAINT_INDICATION",
                        'my_id': nid,
                        'key': accusation_querykey,
                        'accusing_you': yesorno}
        # data_to_send = {'my_id':id, 'rho_commits': str(rho_commits), 'share':str(S[pid])}
        data_to_send = json.dumps(data_to_send)
        DPRINT(data_to_send)
        send_data(connections[pid], data_to_send)


def handleBrocastComplaints(n, nid):
    for pid in peers.keys():
        try:
            broadcastedComplaint = recv_data(peers[pid])
        except:
            print("Exception in handle braodcast with pid:", pid)
            continue

        else:
            broadcastedComplaint = json.loads(broadcastedComplaint)

            if (broadcastedComplaint['msg_type'] == "COMPLAINT_INDICATION"):
                am_I_accused = broadcastedComplaint['accusing_you']
                if (am_I_accused == "yes"):
                    accused_by_id = broadcastedComplaint['my_id']
                    # Now broadcast shares sent to the node of accused_by_id

                    global tx_count
                    global epoch
                    tx_count = tx_count + 1

                    broadcast_sent_shares = records[accused_by_id]['SENT_SHARES']
                    tobdx = {'my_id': nid, 'Reply2Complaint_SentShares': broadcast_sent_shares, 'epoch': 0,
                             'accused_by_id': accused_by_id}
                    replykey = "From" + str(nid) + "Reply2AccusationBy" + str(accused_by_id) + "tx_count" + str(
                        tx_count) + "epoch" + str(epoch) + str(time.strftime("%Y-%m-%d-%H-%M"))

                    ############ Tendermint Broadcast#########
                    # broadcast(broadcast_sent_shares, replykey)
                    broadcast(tobdx, replykey)

                    # Again send this to everyone 

                    data_to_send = {'msg_type': "REPLY2COMPLAINT",
                                    'my_id': nid,
                                    'key': replykey,
                                    'accused_by_id': accused_by_id
                                    }
                    # data_to_send = {'my_id':id, 'rho_commits': str(rho_commits), 'share':str(S[pid])}
                    data_to_send = json.dumps(data_to_send)
                    DPRINT(data_to_send)
                    send_data(connections[pid], data_to_send)

                # Check validity of the accusation                 

                accusation_querykey = broadcastedComplaint['key']
                ###### query from Tendermint  
                complaints[pid] = query(accusation_querykey)

                DPRINT(complaints[pid])

                complaintVerify(complaints[pid], n, nid)

                # TODO: This part can be modular - can reuse code from verifying shares


def complaintVerify(complaints, n, nid):
    accuser_nid = complaints['my_id']  # Node that raised a complaint, we need to use his rows
    accusations = complaints['accusation']
    accusations = json.loads(accusations)  # List of dictionaries - Many nodes against whom accuser sends a complaint

    node_share_index = json.load(open("../../secretsharing/combinatorial/tmp/css_node_share_index.txt"))
    # M_rows = M[node_share_index[str(accuser_nid)]]

    for accstn in accusations:
        DPRINT("accstn:", accstn)
        accused_nid = accstn['node_id']

        # Global record of all accused nodes
        accused_nodes.append(accused_nid)
        # TODO: Dont verify shares if you are not accused??

        # **********************
        return

        # ********************

        if (accused_nid == nid):
            continue
        accuser_shares = deserializeElements(accstn['shares'])
        accuser_shares_dash = deserializeElements(accstn['shares_dash'])

        print("accused_nid:", accused_nid)
        peer_commits = peer_share_commits[accused_nid]

        # g_rand = group.random(G)
        # g = group.encode(decoded_g)
        # h = group.encode(decoded_h)

        verified_shares_counter = 0

        # Check each received share
        for i in range(len(accuser_shares)):
            computed_share_commitment = (g ** accuser_shares[i]) * (h ** accuser_shares_dash[i])

            if computed_share_commitment in peer_commits:
                DPRINT("Share[", i, "] Verified")
                verified_shares_counter += 1

            '''
            commitment_product = g_rand/g_rand #Initialize to unity element

            for j in range(len(M_rows[i])) :
                b = group.init(ZR, int(M_rows[i][j]))
                commitment_product = commitment_product * (peer_rho_commits[j] ** b)

            print("computed_share_commitment:", computed_share_commitment, "commitment_product", commitment_product) 
            if (computed_share_commitment == commitment_product):
                print("Share[",i,"] Verified")
                verified_shares_counter += 1
            '''

        if (verified_shares_counter == len(accuser_shares)):
            print("Great, all shares verified for peer ID:", accused_nid)
            print("Its a wrong accusation!!")
            global QualifiedSet, DisqualifiedSet
            QualifiedSet.append(accused_nid)
        else:
            DisqualifiedSet.append(accused_nid)


def handleBrocastReplies(n, nid):
    print("Handling Broadcast Replies")
    # print("accused_nodes", accused_nodes)
    for pid in peers.keys():
        if pid in set(accused_nodes):
            try:
                broadcastedReply = recv_data(peers[pid])
                DPRINT("broadcastedReply:", broadcastedReply)
            except:
                print("Exception in handling broadcast replies to complaints at peer id:", pid)
                continue

            else:
                broadcastedReply = json.loads(broadcastedReply)

                if (broadcastedReply['msg_type'] == "REPLY2COMPLAINT"):

                    querykey = broadcastedReply['key']

                    reply2complaint = query(querykey)

                    accuser_nid = reply2complaint['accused_by_id']
                    accused_nid = reply2complaint['my_id']

                    accuser_shares_sent = json.loads(reply2complaint['Reply2Complaint_SentShares'])

                    DPRINT("accuser_shares_sent:", accuser_shares_sent)

                    accuser_shares = deserializeElements(accuser_shares_sent['share_strings'])
                    accuser_shares_dash = deserializeElements(accuser_shares_sent['share_dash_strings'])

                    peer_commits = peer_share_commits[accused_nid]

                    verified_shares_counter = 0

                    # Check each received share
                    for i in range(len(accuser_shares)):
                        computed_share_commitment = (g ** accuser_shares[i]) * (h ** accuser_shares_dash[i])

                        if computed_share_commitment in peer_commits:
                            DPRINT("Share[", i, "] Verified")
                            verified_shares_counter += 1

                    if (verified_shares_counter == len(accuser_shares)):
                        print("Great, all shares verified for peer ID:", accused_nid)
                        print("Its a wrong accusation!!")
                        global QualifiedSet, DisqualifiedSet
                        QualifiedSet.append(accused_nid)
                        DisqualifiedSet.append(accuser_nid)
                    else:
                        print("Node " + str(accused_nid) + " is indeed malicious!")
                        DisqualifiedSet.append(accused_nid)


def broadcastDLogNIZK(nid):
    dlog_commit_strings = generated_shares['DlogCommitStrings']
    pedersen_commit_strings = generated_shares['PedersenCommitStrings']
    share_strings = generated_shares['S_strings']
    share_dash_strings = generated_shares['S_dash_strings']

    DPRINT("share_strings:", share_strings)
    dlog_commits = deserializeElements(dlog_commit_strings)
    pedersen_commits = deserializeElements(pedersen_commit_strings)
    shares = deserializeElements(share_strings)
    shares_dash = deserializeElements(share_dash_strings)

    DPRINT("shares:", shares)
    DPRINT("shares_dash:", shares_dash)
    zkp_vec = nizkpok_vec(dlog_commits, pedersen_commits, shares, shares_dash)

    tobdx = {
        'msg_type': 'DLOGNIZK',
        'my_id': nid,
        'DLogStrings': str(dlog_commit_strings),
        'NIZK': str(zkp_vec)
    }

    global tx_count
    global epoch
    tx_count = tx_count + 1
    querykey = "NIZKID" + str(nid) + "tx_count" + str(tx_count) + "epoch" + str(epoch) + str(
        time.strftime("%Y-%m-%d-%H-%M"))

    # Tendermint broadcast
    broadcast(tobdx, querykey)

    data_to_send = {
        'msg_type': 'DLogNizkKey',
        'my_id': nid,
        'key': querykey
    }
    data_to_send = json.dumps(data_to_send)

    # Individual key send
    for pid in list(connections.keys()):
        send_data(connections[pid], data_to_send)


def handleDlogNizk(nid):
    for pid in peers.keys():
        try:
            broadcastedDlogNizk = recv_data(peers[pid])
        except:
            print("Exception: Have not received NIZK from node:", pid)
            continue

        else:
            broadcastedDlogNizk = json.loads(broadcastedDlogNizk)

            if (broadcastedDlogNizk['msg_type'] == "DLogNizkKey"):
                nizk_nid = broadcastedDlogNizk['my_id']
                nizk_querykey = broadcastedDlogNizk['key']

                ###### query from Tendermint  
                nizks[pid] = query(nizk_querykey)

                verifyDlogNizk(nizks[pid], pid)
    return


def verifyDlogNizk(nizks, pid):
    if (nizks['msg_type'] == "DLOGNIZK"):
        print("The received message is dlognizk")
    DPRINT("nizks received:", nizks)
    nizk_nid = nizks['my_id']
    dlog_strings = nizks['DLogStrings']
    nizk_vec = nizks['NIZK']

    # print("nizk_vec",nizk_vec,"type(nizk_vec):", type(nizk_vec), "nizk_vec[0]", nizk_vec[0])

    nizk_vec = deserializeElements(nizk_vec)
    dlog_commits = deserializeElements(dlog_strings)
    pedersen_commits = peer_share_commits[pid]

    # Add the first dlog commitment as public key share needed
    peer_dlog_commits[pid] = dlog_commits[0]

    # g = group.encode(decoded_g)
    # h = group.encode(decoded_h)

    DPRINT("Len of pedersen commits:", len(pedersen_commits))
    DPRINT("Len of dlog     commits:", len(dlog_commits))
    DPRINT("Len of nizk_vec:", len(nizk_vec))

    proofs = []
    for i in range(len(nizk_vec) // 3):
        c = nizk_vec[3 * i]
        u1 = nizk_vec[(3 * i) + 1]
        u2 = nizk_vec[(3 * i) + 2]
        DPRINT("\n\nsent proof:", [c, u1, u2])
        proofs.append([c, u1, u2])  # Putting them back as lists, not sure if it is needed

        V1_dash = (g ** u1) * (dlog_commits[i] ** c)
        dlog_commit_inv = dlog_commits[i] ** (-1)
        V2_dash = (h ** u2) * ((pedersen_commits[i] * dlog_commit_inv) ** c)

        c_dash = group.hash((g, h, dlog_commits[i], pedersen_commits[i], V1_dash, V2_dash), ZR)

        DPRINT("\n\nc:", c)
        # DPRINT("c_dash:",c_dash, "dlog_commit:", dlog_commits[i], "pedersen_commit:", pedersen_commits[i], "V1_dash:", V1_dash, "V2_dash", V2_dash)
        if (c == c_dash):
            print("The NIZK proof is verified")
        else:
            global DisqualifiedSet
            DisqualifiedSet.append(pid)


def dprf_mode(nid, DPRF_PORT):
    ctx = initSSLContext()
    s = SSL.Connection(ctx, socket.socket(socket.AF_INET, socket.SOCK_STREAM))
    s.bind(('', DPRF_PORT))
    s.listen(N_NODES + 1)  # TODO: Make it just one

    client_con, client_addr = s.accept()
    request = recv_data(client_con)
    print("Received request from the client at ", str(client_addr))

    request = json.loads(request)
    print(request)
    X = request['publicstring']
    keytype = request['keytype']

    # par_key will be a list
    par_key = partial_eval(nid, X, keytype)
    serial_par_key = [group.serialize(key) for key in par_key]
    print("serial_par_key:", serial_par_key)

    data_to_send = {
        'my_id': nid,
        'partialEval': str(serial_par_key)
    }

    data_to_send = json.dumps(data_to_send)
    # data_to_send = "Server"+ str(nid) + "Says: hi"
    # oclient_con.sendall("Server Says:hi"+str(nid))
    send_data(client_con, data_to_send)


def computePubKey(nid):
    publicKey = group.random(G)
    publicKey = publicKey / publicKey  # just make it unity

    for pid in peers.keys():
        if pid not in DisqualifiedSet:
            publicKey = publicKey * peer_dlog_commits[pid]

    return publicKey


def node_thread(id):
    DPRINT("Starting Node: ", id)
    MY_PORT = BASE_PORT + id
    DPRF_PORT = MY_PORT + 1000
    # start server part of node to receive Hello

    node_server_thread = threading.Thread(target=serverSock, args=(MY_IP, MY_PORT))
    node_server_thread.daemon = False
    node_server_thread.start()

    sleep_time = (N_NODES - int(args.id))
    sleep(sleep_time)

    # start client part to send hello to all other peers
    node_client_thread = threading.Thread(target=sendId2peers, args=(id,))
    node_client_thread.daemon = False

    node_client_thread.start()

    node_server_thread.join()
    node_client_thread.join()

    DPRINT("Finished the first handshake with all the nodes")

    n = N_NODES

    t_start = time.time()

    print("\nPHASE1: Sending shares to nodes")
    sharing_start = time.process_time()

    share_send_thread = threading.Thread(target=sendShareCommits2Peers, args=(n, id))
    share_send_thread.daemon = False

    share_send_thread.start()
    share_send_thread.join()

    sharing_end = time.process_time()

    # Receive shares
    # sleep(1)
    print("PHASE1: Receiving shares from nodes")

    receive_start = time.process_time()

    share_receive_thread = threading.Thread(target=receive_shares, args=( ))
    share_receive_thread.daemon = False

    share_receive_thread.start()

    share_send_thread.join()
    share_receive_thread.join()

    receive_end = time.process_time()

    print("\nPHASE2: Verifying received commitments ")
    # Verify commitments
    sleep(1)

    verify_start = time.process_time()

    share_verify_thread = threading.Thread(target=verify_received_shares, args=(n, id))
    share_verify_thread.daemon = False

    share_verify_thread.start()
    share_verify_thread.join()

    verify_end = time.process_time()
    print("\nPHASE2: Verified received commitments ")

    '''
    #Broadcast complaints
    print("\nPHASE3: Broadcasting complaints")

    complaint_start = time.process_time()

    broadcastComplaint_thread = threading.Thread(target=broadcastFailedNodeList, args=(id,))
    broadcastComplaint_thread.daemon = False

    broadcastComplaint_thread.start()
    broadcastComplaint_thread.join()

    complaint_end = time.process_time()

    #Handle all complaints (including on self)
    if not MALICIOUS:
        sleep(0.5)
    print("\nPHASE4: Verifying received complaints")

    handle_complaint_start = time.process_time()

    handleComplaint_thread = threading.Thread(target=handleBrocastComplaints, args=(n, id ))
    handleComplaint_thread.daemon = False

    handleComplaint_thread.start()
    handleComplaint_thread.join()

    handle_complaint_end = time.process_time()

    print("\nPHASE4: Verifying replies received  against complaints")

    handle_reply_start = time.process_time()

    handleReply_thread = threading.Thread(target=handleBrocastReplies, args=(n, id ))
    handleReply_thread.daemon = False

    handleReply_thread.start()
    handleReply_thread.join()

    handle_reply_end = time.process_time()


    # Compute own share from qualified set
    if id != 0:
        my_secret_share = [0]*len(my_rcvd_shares[0])
    else:
        my_secret_share = [0]*len(my_rcvd_shares[1])   #Assuming atleast one other node exists 


    for key in my_rcvd_shares.keys():
        if key not in DisqualifiedSet:
           my_secret_share = list(map(add, my_secret_share, my_rcvd_shares[key])) 

    print("my_secret_share", my_secret_share)
    my_share_strings = [str(group.serialize(share)) for share in my_secret_share] 
    
    #Write secret share to a file 
    share_filename = "./tmp/node" + str(id) + "share.txt"
    json.dump(my_share_strings, open(share_filename,'w'))
    '''

    print("\nPHASE5: Broadcasting NIZKs")

    # Broadcast NIZK
    gen_nizk_start = time.process_time()

    broadcastDlogNizk_thread = threading.Thread(target=broadcastDLogNIZK, args=(id,))
    broadcastDlogNizk_thread.daemon = False

    broadcastDlogNizk_thread.start()
    broadcastDlogNizk_thread.join()

    gen_nizk_end = time.process_time()

    # Handle nizk
    print("\nPHASE6: Verifying received NIZKs")
    verify_nizk_start = time.process_time()

    handleNizk_thread = threading.Thread(target=handleDlogNizk, args=(id,))
    handleNizk_thread.daemon = False

    handleNizk_thread.start()
    handleNizk_thread.join()

    verify_nizk_end = time.process_time()

    t_end = time.time()

    total_clock_time = t_end - t_start

    '''
    #Run DPRF   
    share_verify_thread = threading.Thread(target=dprf_mode, args=(id,DPRF_PORT ))
    share_verify_thread.daemon = False

    share_verify_thread.start()
    share_verify_thread.join()
    '''

    sharing_time = (sharing_end - sharing_start) * 1000
    receive_time = (receive_end - receive_start) * 1000
    verify_commit_time = (verify_end - verify_start) * 1000
    # complaint_time        = (complaint_end        - complaint_start)        * 1000
    # complaint_handle_time = (handle_complaint_end - handle_complaint_start) * 1000
    # reply_handle_time     = (handle_reply_end     - handle_reply_start)     * 1000
    gen_nizk_time = (gen_nizk_end - gen_nizk_start) * 1000
    verify_nizk_time = (verify_nizk_end - verify_nizk_start) * 1000

    print("sharing_time:", sharing_time)
    print("receive_time:", receive_time)
    print("verify_commit_time:", verify_commit_time)
    # print( "complaint_time:",        complaint_time)
    # print( "complaint_handle_time:", complaint_handle_time)
    # print( "reply_handle_time:",     reply_handle_time)
    print("gen_nizk_time:", gen_nizk_time)
    print("verify_nizk_time:", verify_nizk_time)

    print("\nPHASEX: Finished the DKG process")

    timing_output = [sharing_time, receive_time, verify_commit_time, gen_nizk_time, verify_nizk_time]

    total_times = [sum(timing_output), total_clock_time]
    print("total_times:", total_times)

    if group == group192:
        bits = 192
    elif group == group256:
        bits = 256
    elif group == group283:
        bits = 283
    elif group == group571:
        bits = 571

    if MALICIOUS:
        timingfilename = "./tmp/css_dkgtiming_malicious_" + str(bits) + "bit_n_" + str(N_NODES) + ".csv"
    else:
        timingfilename = "./tmp/css_non_mal_dkgtiming" + str(bits) + "_n_" + str(N_NODES) + ".csv"
        totaltimingfilename = "./tmp/css_non_mal_dkgtiming_total" + str(bits) + "_n_" + str(N_NODES) + ".csv"

    # out = open(timingfilename, 'a')
    # for column in timing_output:
    #    out.write('%d;' % column)
    #    out.write('\n')
    # out.close()

    with open(timingfilename, "a") as f:
        writer = csv.writer(f)
        writer.writerow(timing_output)

    with open(totaltimingfilename, "a") as tf:
        writer = csv.writer(tf)
        writer.writerow(total_times)


if __name__ == "__main__":

    description = """ 
    This program provides a single node running the DKG instance 
    """

    parser = argparse.ArgumentParser(description=description)
    parser.add_argument(
        "-i", "--id", default=0, type=int, help="node id "
    )

    parser.add_argument(
        "-n", "--nodes", default=4, type=int, help="number of nodes in DKG"
    )

    parser.add_argument(
        "-m", "--malicious", default=0, type=int, help="is the node malicious"
    )

    args = parser.parse_args()

    global N_NODES
    N_NODES = args.nodes
    MALICIOUS = args.malicious

    if MALICIOUS:
        print("\nI am a MALICIOUS node\n\n")

    node_thread(int(args.id))

    sys.exit(0)
