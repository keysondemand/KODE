import sys , json, re , time, csv   
import os, threading, socket, ast
import numpy as np 

from OpenSSL                    import SSL, crypto
from sys                        import argv 
from time                       import sleep
from operator                   import add

from charm.core.engine.util     import *
#from charm.core.math.integer    import *

sys.path += ['./','../','../../']

#KODE related configs and utils 
from conf.connectionconfig                      import *
from conf.groupparam                            import *
from util.connectionutils                       import *
from util.transactionbroadcast                  import *
from util.nizk                                  import *
from secretsharing.blackbox.bbssutil.rhocommit  import *

#from dprf              import partial_eval

debug = 0

MALICIOUS = 0
broadcast_counter = 0

peers = {}
connections = {}

my_rcvd_shares               = {}
my_rcvd_shares_dash          = {}
my_rcvd_shares_strings       = {}
my_rcvd_shares_dash_strings  = {}

peer_share_commits           = {}
peer_dlog_commits            = {}

generated_shares             = {}

complaints                   = {}
records                      = {}
nizks                        = {}


#TODO: Change these to sets 
accused_nodes                = []

nodes_verification_failed    = []
QualifiedSet                 = []
DisqualifiedSet              = []

tx_count = 0
epoch    = 0

#all_client_threads = []

#N_M_map          = json.load(open("../../secretsharing/blackbox/tmp/#N_M_map.txt"))    
node_share_index = json.load(open("../../secretsharing/blackbox/tmp/node_share_index.txt"))    

#complaint_count = {}

def DPRINT ( *args , **kwargs ) :
    if debug:
        print ( *args , **kwargs )

def deserializeElements(objects):
    object_byte_strings = re.findall(r"'(.*?)'", objects , re.DOTALL)
    object_strings  = [ str.encode(a) for a in object_byte_strings]
    elements = [group.deserialize(a) for a in object_strings]
    return elements

def handle_peer(peer_con, nid):

    data_received = recv_data(peer_con)

    if not data_received:
        return 

    #Send ACK 
    peer_con.sendall(b"ACK")
    peer_con.close()

    data_received = json.loads(data_received)
    pid = data_received["my_id"]

    if data_received["msg_type"] == "HELLO":
        print("Hello received")

    if data_received["msg_type"] == "SHARES":
        print("Received Shares")
        receive_shares(nid, pid, data_received)

    if data_received["msg_type"] == "DLogNizkKey":
        print("DLogNizk")
        handleDlogNizk(pid, data_received)
    return 

def serverSock(MY_IP, MY_PORT, nid):

    all_client_threads = []
    s  = socket.socket ( socket.AF_INET , socket.SOCK_STREAM )
    s.bind ( ( '', MY_PORT ) )
    s.listen ( N_NODES)

    t = threading.currentThread()
    while getattr(t, "data_receive", True):
        try:
            peer_con , peer_addr = s.accept ( ) 

            handle_peer_thread = threading.Thread(target=handle_peer, args=(peer_con, nid))
            handle_peer_thread.start()

            all_client_threads.append(handle_peer_thread)
        except Exception as e: print(e)
    print("***Exiting the loop")    

    for thread in all_client_threads:
        thread.join()
    return         


def sendId2peers(nid):

    data_to_send = {'msg_type':"HELLO", 
                    'my_id':nid
                    } 
    data_to_send = json.dumps(data_to_send)

    for pid in range(N_NODES):
        if nid != pid:
            send2Node(nid, pid, data_to_send)
    

def sendShareCommits2Peers(M, nid):
    global tx_count 
    global epoch 
    tx_count = tx_count + 1

    if MALICIOUS:
        S, S_dash , rho_commits, rho_commit_strings, RHO, RHO_dash, dlog_commits, dlog_commit_strings  = rhoCommit(M, MALICIOUS)
    else:
        S, S_dash , rho_commits, rho_commit_strings, RHO, RHO_dash, dlog_commits, dlog_commit_strings  = rhoCommit(M)

    querykey = "ID"+str(nid)+"tx_count"+str(tx_count)+"epoch"+str(epoch) + str(time.strftime("%Y-%m-%d-%H-%M"))

    DPRINT (querykey)

    RHO_strings = []
    RHO_dash_strings = []
    for i in range(len(RHO)):
        RHO_strings.append(group.serialize(RHO[i]))
        RHO_dash_strings.append(group.serialize(RHO_dash[i]))

    #Save random shares generated by self 
    generated_shares['PedersenCommitStrings']  = str(rho_commit_strings)
    generated_shares['RHOStrings']             = str(RHO_strings)
    generated_shares['RHOS']                   = RHO_strings
    generated_shares['RHODashStrings']         = str(RHO_dash_strings)
    generated_shares['DlogCommitStrings']      = str(dlog_commit_strings)

    generated_shares['DlogCom']                = dlog_commits
    generated_shares['RHO']                    = RHO
    generated_shares['RHODash']                = RHO_dash
    generated_shares['PederCom']               = rho_commits 


    ############# Broadcast using Tendermint #####################
    tobdx= {
            'my_id':nid, 
            'BroadcastCommit':str(rho_commit_strings), 
            'epoch': 0
            } 


    broadcast(tobdx, querykey)
    
    DPRINT("S",S)
    DPRINT(rho_commits)
    DPRINT(RHO)
    DPRINT("node_share_index:", node_share_index)
    DPRINT("node_share_index.keys():", node_share_index.keys())

    for pid in range(N_NODES):

        if nid == pid:
            continue 

        records[pid] = {}

        DPRINT("pid:", pid)
        DPRINT("node_share_index[pid]", node_share_index[str(pid)])

        shares               = []
        shares_dash          = []

        shares_strings       = []
        shares_dash_strings  = []

        for index in node_share_index[str(pid)]:
            shares.append(S[index])
            shares_dash.append(S_dash[index])

            shares_strings.append(group.serialize(S[index]))
            shares_dash_strings.append(group.serialize(S_dash[index]))

        data_to_send = {
                        'msg_type':"SHARES", 
                        'my_id':nid, 
                        'key':querykey,
                        'share_strings':str(shares_strings), 
                        'share_dash_strings':str(shares_dash_strings) 
                        }
        data_to_send = json.dumps(data_to_send)

        #Store what is being sent for later usage during complaints
        records[pid]['SENT_SHARES'] = data_to_send
        
        DPRINT (data_to_send)
        try:
            print("Sending shares to node id:", pid)
            send2Node(nid, pid, data_to_send)

        except Exception as e: print("Exception while sending shares:", e)


def receive_shares(nid, pid, share_rcvd):

    try:
        begin_receive_handling = time.process_time()
        
        '''
        peer_share_commits[pid]= ast.literal_eval(share_rcvd['rho_commits'])
        '''
        
        #Store in strings from for complaint phase
        my_rcvd_shares_strings[pid] = share_rcvd['share_strings']
        my_rcvd_shares_dash_strings[pid]= share_rcvd['share_dash_strings']
        
        #Deserialize to obtain the values
        my_rcvd_shares[pid]= deserializeElements(share_rcvd['share_strings'])
        my_rcvd_shares_dash[pid]= deserializeElements(share_rcvd['share_dash_strings'])
        
        query_key = share_rcvd['key'] 
        #############Query from Tendermint############

        query_retries = 0
        while query_retries < 10:
            try:
                queried_result = query(query_key)

            except:
                sleep(1)
                query_retries += 1

            else:
                break 
        DPRINT ("queried_result:", queried_result)
        
        commits = queried_result['BroadcastCommit']
        final_commits = deserializeElements(commits)
        
        DPRINT(final_commits)
        DPRINT("\nExtracted the share", my_rcvd_shares[pid])
        peer_share_commits[pid] = final_commits 
    except Exception as e: print(e)
    verify_received_shares(nid, pid)


def verify_received_shares(nid, pid):
    M_my_rows = M[node_share_index[str(nid)]]
    DPRINT ("My M rows:", M_my_rows)
    try:
        peer_rho_commits =  peer_share_commits[pid]
        shares_rcvd      =  my_rcvd_shares[pid]
        shares_dash_rcvd =  my_rcvd_shares_dash[pid]

        if len(M_my_rows) != len(shares_rcvd):
            print("Eroor!: The number of nodes' rows in M and number of shares received are not same")

        DPRINT("shares_rcvd", shares_rcvd)
        DPRINT("shares_dash_rcvd", shares_dash_rcvd)

        verified_shares_counter = 0

        #Check each received share
        for i in range(len(shares_rcvd)):
            DPRINT("M_my_rows[",i,"]:", M_my_rows[i])
            DPRINT("peer_rho_commits", peer_rho_commits)
            computed_share_commitment = (g ** shares_rcvd[i]) * ( h ** shares_dash_rcvd[i])

            commitment_product = unity #Initialize to unity element  
            if debug:
                DPRINT("Initial commitment product", commitment_product)
            
            for j in range(len(M_my_rows[i])) :
                if M_my_rows[i][j] == 1:
                    #b = group.init(ZR, int(M_my_rows[i][j]))
                    commitment_product = commitment_product * peer_rho_commits[j] 
            
            DPRINT("computed_share_commitment:", computed_share_commitment, "\ncommitment_product", commitment_product)
            if (computed_share_commitment == commitment_product):
                DPRINT("Share[",i,"] Verified")
                verified_shares_counter += 1
            else:
                DPRINT("Share[",i,"] Not Verified")

            
        if(verified_shares_counter == len(shares_rcvd)):
            DPRINT("Great, all shares verified for peer ID:",pid )
            print("Great, all shares verified for peer ID:",pid )
        else:
            print("Something looks fishy, raising a complaint against peer ID:", pid)
            nodes_verification_failed.append(pid)         
    except Exception as e: print("Error during verification of shares:", e)


def broadcastDLogNIZK(nid):
    
    share_not_generated = 1
    while share_not_generated:
        try:
            dlog_commit  = generated_shares['DlogCom'][0]
        except:
            sleep(0.2)
        else:
            share_not_generated = 0

    #dlog_commit             = generated_shares['DlogCom'][0]
    pedersen_commit         = generated_shares['PederCom'][0]
    RHO_zero                = generated_shares['RHO'][0]
    RHO_dash_zero           = generated_shares['RHODash'][0]

    dlog_commit_to_send = [group.serialize(dlog_commit)]

    zkp_vec = nizkpok_vec([dlog_commit], [pedersen_commit], [RHO_zero], [RHO_dash_zero])

    tobdx   = { 
            'msg_type'    : 'DLOGNIZK',
            'my_id'       : nid,
            'DLogStrings' : str(dlog_commit_to_send),
            'NIZK'        : str(zkp_vec)
            }   

    global tx_count
    global epoch
    tx_count = tx_count + 1 
    querykey = "NIZKID" + str(nid) + "tx_count" + str(tx_count) + "epoch" + str(epoch) + str(time.strftime("%Y-%m-%d-%H-%M"))
    
    #Tendermint broadcast 
    broadcast(tobdx, querykey)

    data_to_send = { 
            'msg_type': 'DLogNizkKey',
            'my_id'   : nid,
            'key'     : querykey
            }
    data_to_send = json.dumps(data_to_send)

    #Individual key send
    for pid in range(N_NODES):
        if pid == nid:
            continue 
        try:
            print("Sending Nizk query key to node id:", pid)
            send2Node(nid, pid, data_to_send)
        except Exception as e: print("Error in sending DLogNizk Query Key to node-", pid, e)


def handleDlogNizk(pid, broadcastedDlogNizk):

    nizk_nid      = broadcastedDlogNizk['my_id']
    nizk_querykey = broadcastedDlogNizk['key']
    
    ###### query from Tendermint
    nizks[pid] = query(nizk_querykey)
    
    verifyDlogNizk(nizks[pid], pid)

def verifyDlogNizk(nizks, pid):
    DPRINT("nizks received:", nizks)
    #print("nizks received:", nizks)
    nizk_nid     = nizks['my_id']
    dlog_strings = nizks['DLogStrings']
    nizk_vec     = nizks['NIZK']

    nizk_vec         = deserializeElements(nizk_vec)
    dlog_commits     = deserializeElements(dlog_strings)

    share_not_verified = 1

    #if share is not verified on the other thread yet 
    while share_not_verified:
        try:
            pedersen_commits = peer_share_commits[pid]
        except:
            sleep(1)
        else:
            share_not_verified = 0

    peer_dlog_commits[pid] = dlog_commits[0]

    DPRINT("Len of pedersen commits:", len(pedersen_commits))
    DPRINT("Len of dlog     commits:", len(dlog_commits))
    DPRINT("Len of nizk_vec:", len(nizk_vec))

    proofs = []
    for i in range(len(nizk_vec)//3):

        c = nizk_vec[3*i]
        u1 = nizk_vec[(3*i)+1]
        u2 = nizk_vec[(3*i)+2]
        DPRINT("\n\nsent proof:", [c, u1, u2])
        proofs.append([c,u1,u2]) #Putting them back as lists, not sure if it is needed

        V1_dash = (g ** u1) * (dlog_commits[i] ** c)
        dlog_commit_inv = dlog_commits[i] ** (-1)

        V2_dash = (h ** u2) * ((pedersen_commits[i] * dlog_commit_inv)**c)
        #V2_dash = (h ** u2) * ((pedersen_commits[i]/dlog_commits[i])**c)

        c_dash = group.hash((g,h,dlog_commits[i],pedersen_commits[i], V1_dash, V2_dash), ZR)

        DPRINT("\n\nc:",c)
        #DPRINT("c_dash:",c_dash, "dlog_commit:", dlog_commits[i], "pedersen_commit:", pedersen_commits[i], "V1_dash:", V1_dash, "V2_dash", V2_dash)

        #TODO: This is temporary fix, issue with group.hash function - correct later
        c_str = str(c)
        c_str = c_str[:len(c_str)-30]
        c_dash_str = str(c_dash)
        c_dash_str = c_dash_str[:len(c_dash_str)-30]

        if group == group571:
            if c_str == c_dash_str:
                print("The NIZK proof is verified")
            else:
                DisqualifiedSet.append(pid)
        else:
            if (c == c_dash):
                DPRINT("The NIZK proof is verified")
                print("The NIZK proof is verified")
            else:
                DisqualifiedSet.append(pid)


def node_thread(nid):
    #id = nid 
    DPRINT("Starting Node: ", nid)
    MY_PORT = BASE_PORT + nid
    DPRF_PORT = MY_PORT + 1000
    #start server part of node to receive Hello
    
    print("PHASE0: Attempting handshake with all the nodes")

    node_server_thread = threading.Thread(target = serverSock, args = (MY_IP, MY_PORT, nid))
    node_server_thread.start()

    #start client part to send hello to all other peers 
    node_client_thread = threading.Thread(target = sendId2peers, args = (nid, ))
    node_client_thread.start()
    node_client_thread.join()

    DPRINT("PHASE0: Finished the first handshake with all the nodes")
    print("\nCommencing DKG...")

    sharing_start = time.process_time()

    DPRINT("\nPHASE1: Sending shares to nodes")
    share_send_thread = threading.Thread(target=sendShareCommits2Peers, args=(M, nid))

    share_send_thread.start()
    share_send_thread.join()

    sharing_end = time.process_time()


    DPRINT("\nPHASE5: Broadcasting NIZKs")
    #Broadcast NIZK 
    gen_nizk_start = time.process_time()

    broadcastDlogNizk_thread = threading.Thread(target=broadcastDLogNIZK, args=(nid,))
    broadcastDlogNizk_thread.daemon = False

    broadcastDlogNizk_thread.start()

    broadcastDlogNizk_thread.join()

    gen_nizk_end = time.process_time()    

    #Wait till all shares, nizks  are received 
    while not ((len(nizks) == N_NODES-1 ) and (len(my_rcvd_shares) == N_NODES -1)) :
        sleep(0.5)

    node_server_thread.data_receive = False
    #To close the server socket, just making a temp connectioon
    temp = socket.socket(socket.AF_INET,
                  socket.SOCK_STREAM).connect( (MY_IP, MY_PORT))
    node_server_thread.join()

    print("All shares received, writing my share value to a file")
    
    if nid != 0:
        my_secret_share = [0]*len(my_rcvd_shares[0])
    else:
        my_secret_share = [0]*len(my_rcvd_shares[1])   #Assuming atleast one other node exists 


    for key in my_rcvd_shares.keys():
        if key not in DisqualifiedSet:
           my_secret_share = list(map(add, my_secret_share, my_rcvd_shares[key])) 
           my_secret_share_dash = list(map(add, my_secret_share, my_rcvd_shares_dash[key])) 

    DPRINT("my_secret_share", my_secret_share)
    my_share_strings = [str(group.serialize(share)) for share in my_secret_share] 
    my_share_dash_strings = [str(group.serialize(share)) for share in my_secret_share] 
    
    #Write secret share to a file 
    share_filename = "./tmp/node" + str(nid) + "share.txt"
    json.dump(str(my_share_strings), open(share_filename,'w'))
    share_dash_filename = "./tmp/node" + str(nid) + "share_dash.txt"
    json.dump(str(my_share_dash_strings), open(share_dash_filename,'w'))

    '''
    sharing_time          = (sharing_end          - sharing_start)          * 1000
    verify_commit_time    = (verify_end           - verify_start)           * 1000
    gen_nizk_time         = (gen_nizk_end         - gen_nizk_start)         * 1000
    verify_nizk_time      = (verify_nizk_end      - verify_nizk_start)      * 1000

    print("\n")
    print( "sharing_time:",          sharing_time)
    print( "receive_time:",          receive_time)
    print( "verify_commit_time:",    verify_commit_time)
    #print( "complaint_time:",        complaint_time)
    #print( "complaint_handle_time:", complaint_handle_time)
    #print( "reply_handle_time:",     reply_handle_time)
    print( "gen_nizk_time:",         gen_nizk_time)
    print( "verify_nizk_time:",      verify_nizk_time)
    print("\n")

    #timing_output = [sharing_time, receive_time, verify_commit_time, complaint_time, complaint_handle_time, reply_handle_time, gen_nizk_time, verify_nizk_time]
    timing_output = [sharing_time, receive_time, verify_commit_time, gen_nizk_time, verify_nizk_time]

    if group == group160:
        bits = 160
    if group == group192:
        bits = 192
    elif group == group256:
        bits = 256
    elif group == group283:
        bits = 283
    elif group == group571:
        bits = 571

    if MALICIOUS:
        timingfilename = "./tmp/bbss_dkgtiming_malicious_"+str(bits)+"bit_n_"+str(N_NODES)+".csv"
    else:
        timingfilename = "./tmp/bbss_non_mal_dkgtiming"+str(bits)+"_n_"+str(N_NODES)+".csv"
        totaltimingfilename = "./tmp/bbss_non_mal_dkgtiming_total_"+str(bits)+"_n_"+str(N_NODES)+".csv"

    total_clock_time = t_end - t_start 

    total_times = [sum(timing_output), total_clock_time]
    print("total_time:", total_times)
    #out = open(timingfilename, 'a')
    #for column in timing_output:
    #    out.write('%d;' % column)
    #    out.write('\n')
    #out.close()

    with open(timingfilename, "a") as f:
        writer = csv.writer(f)
        writer.writerow(timing_output)

    with open(totaltimingfilename, "a") as tf:
        writer = csv.writer(tf)
        writer.writerow(total_times)


    DPRINT("PHASEX: Finished the DKG process")
    '''
    

if __name__ == "__main__" :

    description = """ 
    This program provides a single node running the DKG instance 
    """

    parser = argparse.ArgumentParser(description=description)
    parser.add_argument(
        "-i", "--id", default=0, type=int, help="node id "
    )   

    parser.add_argument(
        "-n", "--nodes", default=4, type=int, help="number of nodes in DKG"
    )   

    parser.add_argument(
        "-m", "--malicious", default=0, type=int, help="is the node malicious"
    )   

    args = parser.parse_args()

    global N_NODES
    global M 

    N_NODES = args.nodes
    MALICIOUS = args.malicious

    if N_NODES < 4:
        M = np.loadtxt("../../secretsharing/blackbox/bbssutil/matrices/m3.txt", dtype=int)
    elif N_NODES < 10:
        M = np.loadtxt("../../secretsharing/blackbox/bbssutil/matrices/m9.txt", dtype=int)
    elif N_NODES < 28:
        M = np.loadtxt("../../secretsharing/blackbox/bbssutil/matrices/m27.txt", dtype=int)

    M = np.array(M)

    node_thread(int(args.id))

    sys.exit(0)
    os.system("exit 0")
    




